package com.sundogsoftware.spark.Ratings

// scalastyle:off println

import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.{Seconds, StreamingContext, Time}


/**
  * Use DataFrames and SQL to count words in UTF8 encoded, '\n' delimited text received from the
  * network every second.
  *
  * Usage: SqlNetworkWordCount <hostname> <port>
  * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
  *
  * To run this on your local machine, you need to first run a Netcat server
  *    `$ nc -lk 9999`
  * and then run the example
  *    `$ bin/run-example org.apache.spark.examples.streaming.SqlNetworkWordCount localhost 9999`
  */

@SerialVersionUID(1000L)
object Functions extends Serializable {
  def stopStreamingContext(ssc : StreamingContext) = {
    new Thread("stop-streaming-context") {
      override def run(): Unit = {
        println("******** Streaming application have not reecived data for 60 mins")
        abort()
      }
    }.start()
  }

  // abort helper
  private def abort(): Unit = {
    //logger.info("stopping Streaming Context right now")
    StreamingContext
      .getActive()
      .fold(
        println("No active Streaming Context found")
      )(_.stop())
    println("Streaming Context is stopped!!!!!!!")
    sys.exit(0)
  }
}


@SerialVersionUID(1000L)
object SqlNetworkWordCount extends Serializable {

  @transient
  def main(args: Array[String]) {

    //StreamingExamples.setStreamingLogLevels()

    // Create the context with a 2 second batch size
    @transient val sparkConf = new SparkConf().setAppName("SqlNetworkWordCount").setMaster("local[4]")
    @transient val ssc = new StreamingContext(sparkConf, Seconds(10))

    Logger.getLogger("org").setLevel(Level.INFO)

    //val lines = ssc.socketTextStream("localhost", 9999)

    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    val lines = ssc.socketTextStream("localhost", 9999, StorageLevel.MEMORY_AND_DISK_SER)

    val words = lines.flatMap(_.split(" "))

    // Convert RDDs of the words DStream to DataFrame and run SQL query
    words.foreachRDD { (rdd: RDD[String], time: Time) =>
      // Get the singleton instance of SparkSession
      @transient val spark = SparkSessionSingleton.getInstance(rdd.sparkContext.getConf)
      import spark.implicits._

      // Convert RDD[String] to RDD[case class] to DataFrame
      val wordsDataFrame = rdd.map(w => Record(w)).toDF()

      // Creates a temporary view using the DataFrame
      wordsDataFrame.createOrReplaceTempView("words")

      // Do word count on table using SQL and print it
      val wordCountsDataFrame =
        spark.sql("select word, count(*) as total from words group by word")
      println(s"========= $time =========")
      wordCountsDataFrame.show()
    }


    def checkIfDataIsNotReceived(stream : DStream[String], windowInterval : Int, slideInterval : Int) = {
      val wordsDStream = words.count.reduceByWindow((x,y) => x + y,
        Seconds(windowInterval),
        Seconds(slideInterval))

      wordsDStream.foreachRDD{
        rdd => rdd.foreach( x => println(x))
          println("Inside each rdd")
          rdd.foreach{
            x =>
              if (x == 0) {
                println("Killing job because there is no data coming in ...")
                SparkSessionSingleton.stopStreamingContext()
              } else {
                println(x)
              }
          }
      }
    }

    val output = checkIfDataIsNotReceived(words, 40, 40)

    /*
    val wordsDStream = words.count.reduceByWindow((x,y) => x + y, Seconds(40), Seconds(40))

    wordsDStream.foreachRDD{
      rdd =>
        @transient val spark = SparkSessionSingleton.getInstance(rdd.sparkContext.getConf)
        import spark.implicits._

        // Convert RDD[String] to RDD[case class] to DataFrame
        val wordsDataFrame = rdd.toDF()

        // Creates a temporary view using the DataFrame
        wordsDataFrame.createOrReplaceTempView("wordCountPerWindow")

        // Do word count on table using SQL and print it
        val wordCountsDataFrame =
          spark.sql("select * from wordCountPerWindow")
        wordCountsDataFrame.show()
    }


    wordsDStream.foreachRDD{
      rdd =>
        if (rdd.count() == 0)
          println("Killing job because there is no data coming in ...")
        SparkSessionSingleton.stopStreamingContext(ssc)
    }
    */

    ssc.checkpoint("/tmp/")
    ssc.start()
    ssc.awaitTermination()
  }
}


/** Case class for converting RDD to DataFrame */
case class Record(word: String)


/** Lazily instantiated singleton instance of SparkSession */

@SerialVersionUID(1000L)
object SparkSessionSingleton extends Serializable {

  def getInstance(sparkConf: SparkConf): SparkSession = {
    @transient val instance: SparkSession = SparkSession
        .builder
        .config(sparkConf)
        .getOrCreate()
    instance
    }


  @transient
  def stopStreamingContext() = {
    new Thread("stop-streaming-context") {
      override def run(): Unit = {
        println("******** Streaming application have not reecived data for 40 mins")
        abort()
      }
    }.start()
  }

  // abort helper
  private def abort(): Unit = {
    //logger.info("stopping Streaming Context right now")
    StreamingContext
      .getActive()
      .fold(
        println("No active Streaming Context found")
      )(_.stop())
    println("Streaming Context is stopped!!!!!!!")
    sys.exit(0)
  }

}

